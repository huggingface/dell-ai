{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dell AI SDK - Getting Started \n",
                "\n",
                "This notebook demonstrates the major functionality of the Dell AI SDK, including:\n",
                "- Authentication and user management\n",
                "- Model listing and details\n",
                "- Platform listing and details\n",
                "- Deployment snippet generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "from dell_ai import DellAIClient\n",
                "from dell_ai.exceptions import DellAIError"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize the Client\n",
                "\n",
                "The Dell AI client can be initialized with or without a token. If no token is provided, it will attempt to load from the Hugging Face token cache."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = DellAIClient()  # optionally pass a token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Check Authentication\n",
                "\n",
                "Verify that the client is properly authenticated and get user information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Check if authenticated\n",
                "    is_auth = client.is_authenticated()\n",
                "    print(f\"Is authenticated: {is_auth}\")\n",
                "\n",
                "    # Get user information\n",
                "    if is_auth:\n",
                "        user_info = client.get_user_info()\n",
                "        print(\"\\nUser Information:\")\n",
                "        for key, value in user_info.items():\n",
                "            print(f\"{key}: {value}\")\n",
                "except DellAIError as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. List Available Models\n",
                "\n",
                "Get a list of all available models and their details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 30 models\n",
                        "\n",
                        "Example Model Details:\n",
                        "Model ID: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "Description: The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
                        "License: llama4\n",
                        "Status: new\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    # Get list of available models\n",
                "    models = client.list_models()\n",
                "    print(f\"Found {len(models)} models\")\n",
                "\n",
                "    # Get details for the first model as an example\n",
                "    if models:\n",
                "        first_model = models[0]\n",
                "        model_details = client.get_model(first_model)\n",
                "        print(\"\\nExample Model Details:\")\n",
                "        print(f\"Model ID: {first_model}\")\n",
                "        print(f\"Description: {model_details.description}\")\n",
                "        print(f\"License: {model_details.license}\")\n",
                "        print(f\"Status: {model_details.status}\")\n",
                "except DellAIError as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. List Available Platforms\n",
                "\n",
                "Get a list of all available platforms and their details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 7 platforms\n",
                        "\n",
                        "Example Platform Details:\n",
                        "SKU ID: xe9680-nvidia-h200\n",
                        "Name: XE9680 Nvidia H200\n",
                        "Server: xe9680\n",
                        "Vendor: Nvidia\n",
                        "GPU Type: H200\n",
                        "GPU RAM: 141G\n",
                        "Total GPU Count: 8\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    # Get list of available platforms\n",
                "    platforms = client.list_platforms()\n",
                "    print(f\"Found {len(platforms)} platforms\")\n",
                "\n",
                "    # Get details for the first platform as an example\n",
                "    if platforms:\n",
                "        first_platform = platforms[0]\n",
                "        platform_details = client.get_platform(first_platform)\n",
                "        print(\"\\nExample Platform Details:\")\n",
                "        print(f\"SKU ID: {first_platform}\")\n",
                "        print(f\"Name: {platform_details.name}\")\n",
                "        print(f\"Server: {platform_details.server}\")\n",
                "        print(f\"Vendor: {platform_details.vendor}\")\n",
                "        print(f\"GPU Type: {platform_details.gputype}\")\n",
                "        print(f\"GPU RAM: {platform_details.gpuram}\")\n",
                "        print(f\"Total GPU Count: {platform_details.totalgpucount}\")\n",
                "except DellAIError as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5 Check Model Platform Support\n",
                "\n",
                "Get information about which platforms and deployment configurations are supported for a specific model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Platform Support for Model: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "==================================================\n",
                        "\n",
                        "Platform: xe9680-amd-mi300x\n",
                        "  Configuration:\n",
                        "    - Max Batch Prefill Tokens: 16484\n",
                        "    - Max Input Tokens: 16383\n",
                        "    - Max Total Tokens: 16384\n",
                        "    - Number of GPUs: 8\n",
                        "\n",
                        "Platform: xe9680-nvidia-h200\n",
                        "  Configuration:\n",
                        "    - Max Batch Prefill Tokens: 8484\n",
                        "    - Max Input Tokens: 8383\n",
                        "    - Max Total Tokens: 8384\n",
                        "    - Number of GPUs: 8\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    # Get details for a specific model\n",
                "    if models:\n",
                "        model_id = models[0]  # Using first model\n",
                "        model_details = client.get_model(model_id)\n",
                "\n",
                "        print(f\"\\nPlatform Support for Model: {model_id}\")\n",
                "        print(\"=\" * 50)\n",
                "\n",
                "        # Check if the model has any deployment configurations\n",
                "        if not model_details.configs_deploy:\n",
                "            print(\"No deployment configurations available for this model.\")\n",
                "        else:\n",
                "            # Print supported platforms and their configurations\n",
                "            for platform_id, configs in model_details.configs_deploy.items():\n",
                "                print(f\"\\nPlatform: {platform_id}\")\n",
                "                for config in configs:\n",
                "                    print(\"  Configuration:\")\n",
                "                    print(\n",
                "                        f\"    - Max Batch Prefill Tokens: {config.max_batch_prefill_tokens}\"\n",
                "                    )\n",
                "                    print(f\"    - Max Input Tokens: {config.max_input_tokens}\")\n",
                "                    print(f\"    - Max Total Tokens: {config.max_total_tokens}\")\n",
                "                    print(f\"    - Number of GPUs: {config.num_gpus}\")\n",
                "except DellAIError as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Get Deployment Snippet (for Docker or Kubernetes)\n",
                "\n",
                "Get a deployment snippet for a specific model and platform configuration. We'll use the supported configuration values from the model details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using configuration for meta-llama/Llama-4-Maverick-17B-128E-Instruct on xe9680-amd-mi300x:\n",
                        "- Max Batch Prefill Tokens: 16484\n",
                        "- Max Input Tokens: 16383\n",
                        "- Max Total Tokens: 16384\n",
                        "- Number of GPUs: 8\n",
                        "\n",
                        " ----------------------------------------------------------------------------------------------------\n",
                        "Docker Deployment Snippet:\n",
                        "docker run \\\n",
                        "    -it \\\n",
                        "    -p 80:80 \\\n",
                        "    --security-opt seccomp=unconfined \\\n",
                        "    --device=/dev/kfd \\\n",
                        "    --device=/dev/dri \\\n",
                        "    --group-add video \\\n",
                        "    --ipc=host \\\n",
                        "    --shm-size 256g \\\n",
                        "    -e NUM_SHARD=8 \\\n",
                        "    -e MAX_BATCH_PREFILL_TOKENS=16484 \\\n",
                        "    -e MAX_TOTAL_TOKENS=16384 \\\n",
                        "    -e MAX_INPUT_TOKENS=16383 \\\n",
                        "    registry.dell.huggingface.co/enterprise-dell-inference-meta-llama-llama-4-maverick-17b-128e-instruct-amd\n",
                        "\n",
                        " ----------------------------------------------------------------------------------------------------\n",
                        "Kubernetes Deployment Snippet:\n",
                        "# Write the Kubernetes manifest below in a deployment.yaml file,\n",
                        "# and then run the following kubectl command on the Kubernetes Cluster:\n",
                        "# kubectl apply -f deployment.yaml\n",
                        "\n",
                        "apiVersion: apps/v1\n",
                        "kind: Deployment\n",
                        "metadata:\n",
                        "  name: tgi-deployment\n",
                        "spec:\n",
                        "  replicas: 1\n",
                        "  selector:\n",
                        "    matchLabels:\n",
                        "      app: tgi-server\n",
                        "  template:\n",
                        "    metadata:\n",
                        "      labels:\n",
                        "        app: tgi-server\n",
                        "        hf.co/model: meta-llama--Llama-4-Maverick-17B-128E-Instruct\n",
                        "        hf.co/task: text-generation\n",
                        "    spec:\n",
                        "      containers:\n",
                        "        - name: tgi-container\n",
                        "          image: registry.dell.huggingface.co/enterprise-dell-inference-meta-llama-llama-4-maverick-17b-128e-instruct-amd\n",
                        "          securityContext:\n",
                        "            seccompProfile:\n",
                        "              type: Unconfined\n",
                        "          resources:\n",
                        "            limits:\n",
                        "              amd.com/gpu: 8\n",
                        "          env: \n",
                        "            - name: NUM_SHARD\n",
                        "              value: \"8\"\n",
                        "            - name: MAX_BATCH_PREFILL_TOKENS\n",
                        "              value: \"16484\"\n",
                        "            - name: MAX_TOTAL_TOKENS\n",
                        "              value: \"16384\"\n",
                        "            - name: MAX_INPUT_TOKENS\n",
                        "              value: \"16383\"\n",
                        "          volumeMounts:\n",
                        "            - mountPath: /dev/shm\n",
                        "              name: dshm\n",
                        "            - name: dev-kfd\n",
                        "              mountPath: /dev/kfd\n",
                        "            - name: dev-dri\n",
                        "              mountPath: /dev/dri\n",
                        "      volumes:\n",
                        "        - name: dshm\n",
                        "          emptyDir:\n",
                        "            medium: Memory\n",
                        "            sizeLimit: 256Gi\n",
                        "        - name: dev-kfd\n",
                        "          hostPath:\n",
                        "            path: /dev/kfd\n",
                        "        - name: dev-dri\n",
                        "          hostPath:\n",
                        "            path: /dev/dri\n",
                        "---\n",
                        "apiVersion: v1\n",
                        "kind: Service\n",
                        "metadata:\n",
                        "  name: tgi-service\n",
                        "spec:\n",
                        "  type: LoadBalancer\n",
                        "  ports:\n",
                        "    - protocol: TCP\n",
                        "      port: 80\n",
                        "      targetPort: 80\n",
                        "  selector:\n",
                        "    app: tgi-server\n",
                        "---\n",
                        "apiVersion: networking.k8s.io/v1\n",
                        "kind: Ingress\n",
                        "metadata:\n",
                        "  name: tgi-ingress\n",
                        "  annotations:\n",
                        "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
                        "spec:\n",
                        "  ingressClassName: nginx-ingress\n",
                        "  rules:\n",
                        "    - http:\n",
                        "        paths:\n",
                        "          - path: /\n",
                        "            pathType: Prefix\n",
                        "            backend:\n",
                        "              service:\n",
                        "                name: tgi-service\n",
                        "                port:\n",
                        "                  number: 80\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    # Get model details to find supported configurations\n",
                "    if models:\n",
                "        model_id = models[0]  # Using first model\n",
                "        model_details = client.get_model(model_id)\n",
                "\n",
                "        # Get the first supported platform and its configuration\n",
                "        if model_details.configs_deploy:\n",
                "            platform_id = next(iter(model_details.configs_deploy))\n",
                "            config = model_details.configs_deploy[platform_id][0]  # Get first config\n",
                "\n",
                "            print(f\"Using configuration for {model_id} on {platform_id}:\")\n",
                "            print(f\"- Max Batch Prefill Tokens: {config.max_batch_prefill_tokens}\")\n",
                "            print(f\"- Max Input Tokens: {config.max_input_tokens}\")\n",
                "            print(f\"- Max Total Tokens: {config.max_total_tokens}\")\n",
                "            print(f\"- Number of GPUs: {config.num_gpus}\")\n",
                "\n",
                "            # Get deployment snippet for Docker\n",
                "            docker_snippet = client.get_deployment_snippet(\n",
                "                model_id=model_id,\n",
                "                sku_id=platform_id,\n",
                "                container_type=\"docker\",\n",
                "                num_gpus=config.num_gpus,\n",
                "                num_replicas=1,\n",
                "            )\n",
                "\n",
                "            print(\"\\n\", \"--\" * 50)\n",
                "            print(\"Docker Deployment Snippet:\")\n",
                "            print(docker_snippet)\n",
                "\n",
                "            # Get deployment snippet for Kubernetes\n",
                "            k8s_snippet = client.get_deployment_snippet(\n",
                "                model_id=model_id,\n",
                "                sku_id=platform_id,\n",
                "                container_type=\"kubernetes\",\n",
                "                num_gpus=config.num_gpus,\n",
                "                num_replicas=1,\n",
                "            )\n",
                "\n",
                "            print(\"\\n\", \"--\" * 50)\n",
                "            print(\"Kubernetes Deployment Snippet:\")\n",
                "            print(k8s_snippet)\n",
                "        else:\n",
                "            print(\"No deployment configurations available for this model.\")\n",
                "except DellAIError as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
